# LLMQL

LLMQL es concepto de proyecto que consulta SQL directamente con lenguaje natural utilizando un modelo local que entiende espa√±ol e ingl√©s, con √©nfasis en seguridad de solo lectura.

## Caracter√≠sticas

- Generaci√≥n de consultas SQL a partir de lenguaje natural.
- Conexi√≥n a MySQL.
- Registro y auditor√≠a de cada consulta generada y ejecutada.
- Modelo local: Permite integrar otro modelo de manera no muy compleja.

## Requisitos

- Python 3.12 o superior
- MySQL o MariaDB

## Instalaci√≥n

1. Clona el repositorio:

    ```bash
    $ git clone https://github.com/sabiopobre/llmql.git
    $ cd llmql
    ```

2. Instala las dependencias:

    ```bash
    $ pip install -r requirements.txt
    ```

3. Configura las variables de entorno en el archivo `.env`:

    ```plaintext
    DB_HOST=localhost
    DB_USER=root
    DB_PASSWORD=your_password
    DB_NAME=llmql
    ```

## Uso

Ejecuta la CLI con una consulta en lenguaje natural:

```bash
$ python consulta_cli.py "Ventas del √∫ltimo mes"
```

## Ejemplo

Supongamos que tienes una tabla llamada `ventas` y quieres saber cu√°ntas ventas hubo el √∫ltimo mes. Puedes ejecutar el siguiente comando:

```bash
$ python consulta_cli.py "¬øCu√°ntas ventas hubo el √∫ltimo mes?"
```

La CLI generar√° la consulta SQL correspondiente y devolver√° el resultado:

```plaintext
Generated SQL: SELECT COUNT(*) FROM ventas WHERE fecha >= DATE_SUB(CURDATE(), INTERVAL 1 MONTH)
Result: 150
```

## Estructura del Proyecto

```
llmql/
‚îÇ
‚îú‚îÄ‚îÄ consulta_cli.py  # CLI principal
‚îú‚îÄ‚îÄ sql_generator.py # Interacci√≥n con el modelo local
‚îú‚îÄ‚îÄ db_connector.py  # Conexi√≥n segura a MySQL
‚îú‚îÄ‚îÄ logger.py        # Logs y auditor√≠a
‚îú‚îÄ‚îÄ .env             # Configuraci√≥n de BD
‚îú‚îÄ‚îÄ requirements.txt # Dependencias del proyecto
‚îî‚îÄ‚îÄ README.md        # Documentaci√≥n del proyecto
```

## Cambiar el Modelo de IA

Actualmente, el proyecto utiliza el modelo `qwen2:7b` a modo de ejemplo. Para que funcione, aseg√∫rate de tener `ollama` activo en tu equipo con el modelo instalado. Puedes ejecutar el siguiente comando para iniciar `ollama` con el modelo `qwen2:7b`:

```bash
$ ollama run qwen2:7b
```

### C√≥digo Actual

```python
import ollama

def generate_sql(query: str) -> str:
    response = ollama.chat(model='qwen2:7b', messages=[
        {'role': 'system', 'content': 'You are a SQL query generator. Translate the following natural language query into a SQL query. The SQL query should start with "SELECT" and be valid SQL syntax.'},
        {'role': 'user', 'content': query}
    ])
    sql = response['message']['content']
    if not sql.strip().upper().startswith("SELECT"):
        raise ValueError(f"La respuesta generada no es una consulta SQL v√°lida: {sql}")
    return sql
```

Para cambiar el modelo de IA utilizado para generar consultas SQL, simplemente actualiza el archivo `sql_generator.py` para utilizar el nuevo modelo. Por ejemplo, si deseas utilizar un modelo diferente de la biblioteca `transformers` de Hugging Face, puedes hacerlo de la siguiente manera:

```python
from transformers import pipeline

# Cargar el modelo de Hugging Face
nlp = pipeline("text2sql", model="nombre_del_modelo")

# Generar la consulta SQL
response = nlp(query)
return response["generated_text"]
```

Este enfoque permite que el proyecto sea agn√≥stico al modelo de IA, facilitando la actualizaci√≥n o el cambio del modelo sin necesidad de modificar el c√≥digo base.

## Licencia

<p align="center">
	Repositorio generado por <a href="https://github.com/virtuanista" target="_blank">virtu üé£</a>
</p>

<p align="center">
	<img src="https://open.soniditos.com/cat_footer.svg" />
</p>

<p align="center">
	Copyright &copy; 2025
</p>

<p align="center">
	<a href="/LICENSE"><img src="https://img.shields.io/static/v1.svg?style=for-the-badge&label=License&message=MIT&logoColor=d9e0ee&colorA=363a4f&colorB=b7bdf8"/></a>
</p>
